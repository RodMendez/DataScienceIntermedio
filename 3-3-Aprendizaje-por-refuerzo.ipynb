{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyzZ12KI8xZS"
      },
      "source": [
        "# Aprendizaje por refuerzo (posponer al viernes)\n",
        "\n",
        "Instalar las siguientes librerias para usar esta notebook:\n",
        "\n",
        "!pip install cmake pygame gym[all] pip stable_baselines3\n",
        "\n",
        "Referencia: https://www.gocoder.one/blog/rl-tutorial-with-openai-gym\n",
        "\n",
        "![](https://raw.githubusercontent.com/igomezv/DataScienceIntermedio/main/img/refuerzo.png)\n",
        "Fuente de la imagen: medium.com\n",
        "\n",
        "![](https://raw.githubusercontent.com/igomezv/DataScienceIntermedio/main/img/RLdog.jpg)\n",
        "Fuente: kdnuggets.com\n",
        "\n",
        "Objetivo: maximizar la recompenza.\n",
        "\n",
        "**Definiciones importantes:**\n",
        "\n",
        "- Agente: modelo a entrenar para que tome decisiones.\n",
        "- Ambiente: Entorno donde interactúa el agente. El ambiente limita y pone reglas.\n",
        "\n",
        "Entre ambiente y agente hay las siguientes relaciones:\n",
        "\n",
        "- Acción: posibles acciones que puede tomar el agente en determinado momento.\n",
        "- Estado (del ambiente): indicadores del ambiente sobre cómo están los diversos elementos que lo componen en determinado momento.\n",
        "- Recompensas (o penalización): cada acción del agente obtiene un premio o una penalización.\n",
        "\n",
        "Ver: https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/igomezv/DataScienceIntermedio/main/img/RLalg.png)\n",
        "Fuente: medium.com\n",
        "\n",
        "#### Usar el entorno TAXI\n",
        "\n",
        "**Contexto:** Taxi es un entorno disponible en Open-IA gym (https://www.gymlibrary.dev/environments/toy_text/taxi/).\n",
        "\n",
        "- El objetivo de Taxi es recoger pasajeros y dejarlos en el destino en la menor cantidad de movimientos.\n",
        "\n",
        "- Se comparara un agente de taxis que realiza acciones al azar con uno entrenado mediante Aprendizaje Reforzado.\n",
        "\n",
        "- Acciones: moverse hacia arriba, abajo, izquierda, derecha, recoger o dejar pasajeros.\n",
        "\n",
        "Descripción de la documentación:\n",
        "\n",
        "Hay cuatro ubicaciones designadas en el mundo de la cuadrícula indicadas por R (rojo), G (verde), Y (amarillo) y B (azul). Cuando el episodio inicia, el taxi parte de una casilla aleatoria y el pasajero se encuentra en una ubicación aleatoria. El taxi conduce hasta la ubicación del pasajero, lo recoge, conduce hasta el destino del pasajero (otra de las cuatro ubicaciones especificadas) y luego deja al pasajero. Una vez que se deja al pasajero, el episodio termina.\n",
        "\n",
        "Map:\n",
        "\n",
        "+---------+\n",
        "\n",
        "|R: | : :G|\n",
        "\n",
        "| : | : : |\n",
        "\n",
        "| : : : : |\n",
        "\n",
        "| | : | : |\n",
        "\n",
        "|Y| : |B: |\n",
        "\n",
        "+---------+\n",
        "\n",
        "##### Comportamiento\n",
        "\n",
        "\n",
        "Hay 6 acciones deterministas discretas:\n",
        "\n",
        "    0: mover al sur\n",
        "\n",
        "    1: mover al norte\n",
        "\n",
        "    2: moverse hacia el este\n",
        "\n",
        "    3: muévete hacia el oeste\n",
        "\n",
        "    4: pasajero de recogida\n",
        "\n",
        "    5: dejar al pasajero\n",
        "\n",
        "Observaciones\n",
        "\n",
        "Hay 500 estados discretos ya que hay 25 posiciones de taxi, 5 ubicaciones posibles del pasajero (incluido el caso cuando el pasajero está en el taxi) y 4 ubicaciones de destino.\n",
        "\n",
        "Tenga en cuenta que hay 400 estados a los que se puede llegar durante un episodio. Los estados faltantes corresponden a situaciones en las que el pasajero se encuentra en el mismo lugar que su destino, ya que esto suele indicar el final de un episodio. Se pueden observar cuatro estados adicionales justo después de un episodio exitoso, cuando tanto el pasajero como el taxi están en el destino. Esto da un total de 404 estados discretos alcanzables.\n",
        "\n",
        "Cada espacio de estado está representado por la tupla: (fila_taxi, col_taxi, ubicación_pasajero, destino)\n",
        "\n",
        "\n",
        "Ubicaciones de pasajeros:\n",
        "\n",
        "    0: R (rojo)\n",
        "\n",
        "    1: G (verde)\n",
        "\n",
        "    2: Y (amarillo)\n",
        "\n",
        "    3: B (azul)\n",
        "\n",
        "    4: en taxi\n",
        "\n",
        "Destinos:\n",
        "\n",
        "    0: R (rojo)\n",
        "\n",
        "    1: G (verde)\n",
        "\n",
        "    2: Y (amarillo)\n",
        "\n",
        "    3: B (azul)\n",
        "\n",
        "Recompensas\n",
        "\n",
        "     -1 por paso a menos que se active otra recompensa.\n",
        "\n",
        "     +20 entregando pasajero.\n",
        "\n",
        "     -10 ejecutar acciones de “recogida” y “devolución” de forma ilegal.\n",
        "     \n",
        "     - Una ilegalidad consiste en ejecutar las acciones\"recoger\"o \"devolver\" en lugares donde no hay personas.\n",
        "\n",
        "![](https://www.gocoder.one/static/state-rewards-62ab43a53e07062b531b3199a8bab5b3.png)\n",
        "\n",
        "Como llegar al punto R sin perder tantos puntos?\n",
        "\n",
        "## Q-Learning\n",
        "\n",
        "### Tablas-Q\n",
        "\n",
        "Una tabla Q es una tabla de búsqueda que almacena valores que representan las recompensas futuras máximas esperadas que el agente puede esperar por una acción determinada en un estado determinado (conocidos como valores Q).\n",
        "\n",
        "![](https://i.stack.imgur.com/Bn6MY.gif)\n",
        "\n",
        "Estas tablas se suelen inicializar en 0s\n",
        "\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "\n",
        "Hyperparámetros:\n",
        "\n",
        "- Tasa de aprendizaje (α): qué tanto el agente debe aceptar nueva información sobre la información previamente aprendida\n",
        "\n",
        "- Factor de descuento (γ): qué tanto el agente debe considerar las recompensas que podría recibir en el futuro frente a su recompensa inmediata.\n",
        "\n",
        "#### Primero, analicemos el comportamiento aleatorio"
      ],
      "id": "zyzZ12KI8xZS"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cmake pygame gym[all]\n",
        "!pip stable_baselines3"
      ],
      "metadata": {
        "id": "aQ4ylQkQ86aV",
        "outputId": "506351d3-e736-4206-a07a-14f3d7187085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aQ4ylQkQ86aV",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (3.27.7)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[all])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mujoco-py<2.2,>=2.1 (from gym[all])\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (3.7.1)\n",
            "Collecting lz4>=3.1.0 (from gym[all])\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (4.8.0.76)\n",
            "Collecting mujoco==2.2.0 (from gym[all])\n",
            "  Downloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (2.31.6)\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.* (from gym[all])\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest==7.0.1 (from gym[all])\n",
            "  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ale-py~=0.7.5 (from gym[all])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[all]) (1.4.0)\n",
            "Collecting glfw (from mujoco==2.2.0->gym[all])\n",
            "  Downloading glfw-2.6.2-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[all]) (3.1.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (1.3.0)\n",
            "Collecting py>=1.8.2 (from pytest==7.0.1->gym[all])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (2.0.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[all]) (6.1.0)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gym[all]) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.16.0)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gym[all])\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[all]) (1.16.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mERROR: unknown command \"stable_baselines3\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZF7JGuc58xZU"
      },
      "outputs": [],
      "source": [
        "import gym"
      ],
      "id": "ZF7JGuc58xZU"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mXZDT7P-8xZU",
        "outputId": "7a0a58d7-1394-480d-9b4f-3cc336ffe508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "id": "mXZDT7P-8xZU"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1BASXRSh8xZV"
      },
      "outputs": [],
      "source": [
        "# create a new instance of taxi, and get the initial state\n",
        "state = env.reset()"
      ],
      "id": "1BASXRSh8xZV"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hs_7GdQ68xZV"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def randomtaxi():\n",
        "    total_reward = 0\n",
        "    # crear entorno de Taxi\n",
        "    env = gym.make('Taxi-v3')\n",
        "\n",
        "    # crear una instancia de Taxi, y obtener un estado inicial\n",
        "    state = env.reset()\n",
        "\n",
        "    num_steps = 99\n",
        "\n",
        "    for s in range(num_steps+1):\n",
        "        print(\"Paso: {}/{}\".format(s+1, num_steps+1))\n",
        "\n",
        "        # muestrear una accion aleatoria de la lista de acciones posibles\n",
        "        action = env.action_space.sample()\n",
        "        # ejecutar esta accion sobre el entorno\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        # Update to our new state\n",
        "        state = new_state\n",
        "        total_reward += reward\n",
        "        print(\"Accion {} Estado {} Recompensa {} Total rec {}\".format(action, state, reward, total_reward))\n",
        "\n",
        "\n",
        "        # imprimir nuevo estado\n",
        "        env.render()\n",
        "\n",
        "        # if done, finish episode\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "\n",
        "    # terminar y cerrar el entorno del taxi\n",
        "    env.close()\n",
        "    return total_reward"
      ],
      "id": "hs_7GdQ68xZV"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9PDkmwqj8xZW",
        "outputId": "6f894815-f415-4549-f57f-0f3b72a10874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paso: 1/100\n",
            "Accion 5 Estado 249 Recompensa -10 Total rec -10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paso: 2/100\n",
            "Accion 1 Estado 149 Recompensa -1 Total rec -11\n",
            "Paso: 3/100\n",
            "Accion 1 Estado 49 Recompensa -1 Total rec -12\n",
            "Paso: 4/100\n",
            "Accion 0 Estado 149 Recompensa -1 Total rec -13\n",
            "Paso: 5/100\n",
            "Accion 2 Estado 169 Recompensa -1 Total rec -14\n",
            "Paso: 6/100\n",
            "Accion 0 Estado 269 Recompensa -1 Total rec -15\n",
            "Paso: 7/100\n",
            "Accion 0 Estado 369 Recompensa -1 Total rec -16\n",
            "Paso: 8/100\n",
            "Accion 5 Estado 369 Recompensa -10 Total rec -26\n",
            "Paso: 9/100\n",
            "Accion 0 Estado 469 Recompensa -1 Total rec -27\n",
            "Paso: 10/100\n",
            "Accion 1 Estado 369 Recompensa -1 Total rec -28\n",
            "Paso: 11/100\n",
            "Accion 3 Estado 369 Recompensa -1 Total rec -29\n",
            "Paso: 12/100\n",
            "Accion 3 Estado 369 Recompensa -1 Total rec -30\n",
            "Paso: 13/100\n",
            "Accion 5 Estado 369 Recompensa -10 Total rec -40\n",
            "Paso: 14/100\n",
            "Accion 3 Estado 369 Recompensa -1 Total rec -41\n",
            "Paso: 15/100\n",
            "Accion 2 Estado 389 Recompensa -1 Total rec -42\n",
            "Paso: 16/100\n",
            "Accion 3 Estado 369 Recompensa -1 Total rec -43\n",
            "Paso: 17/100\n",
            "Accion 0 Estado 469 Recompensa -1 Total rec -44\n",
            "Paso: 18/100\n",
            "Accion 2 Estado 489 Recompensa -1 Total rec -45\n",
            "Paso: 19/100\n",
            "Accion 2 Estado 489 Recompensa -1 Total rec -46\n",
            "Paso: 20/100\n",
            "Accion 2 Estado 489 Recompensa -1 Total rec -47\n",
            "Paso: 21/100\n",
            "Accion 5 Estado 489 Recompensa -10 Total rec -57\n",
            "Paso: 22/100\n",
            "Accion 3 Estado 469 Recompensa -1 Total rec -58\n",
            "Paso: 23/100\n",
            "Accion 5 Estado 469 Recompensa -10 Total rec -68\n",
            "Paso: 24/100\n",
            "Accion 2 Estado 489 Recompensa -1 Total rec -69\n",
            "Paso: 25/100\n",
            "Accion 0 Estado 489 Recompensa -1 Total rec -70\n",
            "Paso: 26/100\n",
            "Accion 1 Estado 389 Recompensa -1 Total rec -71\n",
            "Paso: 27/100\n",
            "Accion 0 Estado 489 Recompensa -1 Total rec -72\n",
            "Paso: 28/100\n",
            "Accion 5 Estado 489 Recompensa -10 Total rec -82\n",
            "Paso: 29/100\n",
            "Accion 1 Estado 389 Recompensa -1 Total rec -83\n",
            "Paso: 30/100\n",
            "Accion 2 Estado 389 Recompensa -1 Total rec -84\n",
            "Paso: 31/100\n",
            "Accion 2 Estado 389 Recompensa -1 Total rec -85\n",
            "Paso: 32/100\n",
            "Accion 3 Estado 369 Recompensa -1 Total rec -86\n",
            "Paso: 33/100\n",
            "Accion 4 Estado 369 Recompensa -10 Total rec -96\n",
            "Paso: 34/100\n",
            "Accion 2 Estado 389 Recompensa -1 Total rec -97\n",
            "Paso: 35/100\n",
            "Accion 4 Estado 389 Recompensa -10 Total rec -107\n",
            "Paso: 36/100\n",
            "Accion 3 Estado 369 Recompensa -1 Total rec -108\n",
            "Paso: 37/100\n",
            "Accion 2 Estado 389 Recompensa -1 Total rec -109\n",
            "Paso: 38/100\n",
            "Accion 3 Estado 369 Recompensa -1 Total rec -110\n",
            "Paso: 39/100\n",
            "Accion 5 Estado 369 Recompensa -10 Total rec -120\n",
            "Paso: 40/100\n",
            "Accion 5 Estado 369 Recompensa -10 Total rec -130\n",
            "Paso: 41/100\n",
            "Accion 0 Estado 469 Recompensa -1 Total rec -131\n",
            "Paso: 42/100\n",
            "Accion 4 Estado 469 Recompensa -10 Total rec -141\n",
            "Paso: 43/100\n",
            "Accion 3 Estado 469 Recompensa -1 Total rec -142\n",
            "Paso: 44/100\n",
            "Accion 4 Estado 469 Recompensa -10 Total rec -152\n",
            "Paso: 45/100\n",
            "Accion 0 Estado 469 Recompensa -1 Total rec -153\n",
            "Paso: 46/100\n",
            "Accion 3 Estado 469 Recompensa -1 Total rec -154\n",
            "Paso: 47/100\n",
            "Accion 3 Estado 469 Recompensa -1 Total rec -155\n",
            "Paso: 48/100\n",
            "Accion 0 Estado 469 Recompensa -1 Total rec -156\n",
            "Paso: 49/100\n",
            "Accion 2 Estado 489 Recompensa -1 Total rec -157\n",
            "Paso: 50/100\n",
            "Accion 5 Estado 489 Recompensa -10 Total rec -167\n",
            "Paso: 51/100\n",
            "Accion 0 Estado 489 Recompensa -1 Total rec -168\n",
            "Paso: 52/100\n",
            "Accion 0 Estado 489 Recompensa -1 Total rec -169\n",
            "Paso: 53/100\n",
            "Accion 5 Estado 489 Recompensa -10 Total rec -179\n",
            "Paso: 54/100\n",
            "Accion 2 Estado 489 Recompensa -1 Total rec -180\n",
            "Paso: 55/100\n",
            "Accion 5 Estado 489 Recompensa -10 Total rec -190\n",
            "Paso: 56/100\n",
            "Accion 4 Estado 489 Recompensa -10 Total rec -200\n",
            "Paso: 57/100\n",
            "Accion 3 Estado 469 Recompensa -1 Total rec -201\n",
            "Paso: 58/100\n",
            "Accion 0 Estado 469 Recompensa -1 Total rec -202\n",
            "Paso: 59/100\n",
            "Accion 0 Estado 469 Recompensa -1 Total rec -203\n",
            "Paso: 60/100\n",
            "Accion 2 Estado 489 Recompensa -1 Total rec -204\n",
            "Paso: 61/100\n",
            "Accion 4 Estado 489 Recompensa -10 Total rec -214\n",
            "Paso: 62/100\n",
            "Accion 0 Estado 489 Recompensa -1 Total rec -215\n",
            "Paso: 63/100\n",
            "Accion 4 Estado 489 Recompensa -10 Total rec -225\n",
            "Paso: 64/100\n",
            "Accion 3 Estado 469 Recompensa -1 Total rec -226\n",
            "Paso: 65/100\n",
            "Accion 3 Estado 469 Recompensa -1 Total rec -227\n",
            "Paso: 66/100\n",
            "Accion 1 Estado 369 Recompensa -1 Total rec -228\n",
            "Paso: 67/100\n",
            "Accion 1 Estado 269 Recompensa -1 Total rec -229\n",
            "Paso: 68/100\n",
            "Accion 3 Estado 249 Recompensa -1 Total rec -230\n",
            "Paso: 69/100\n",
            "Accion 4 Estado 249 Recompensa -10 Total rec -240\n",
            "Paso: 70/100\n",
            "Accion 1 Estado 149 Recompensa -1 Total rec -241\n",
            "Paso: 71/100\n",
            "Accion 5 Estado 149 Recompensa -10 Total rec -251\n",
            "Paso: 72/100\n",
            "Accion 1 Estado 49 Recompensa -1 Total rec -252\n",
            "Paso: 73/100\n",
            "Accion 0 Estado 149 Recompensa -1 Total rec -253\n",
            "Paso: 74/100\n",
            "Accion 5 Estado 149 Recompensa -10 Total rec -263\n",
            "Paso: 75/100\n",
            "Accion 1 Estado 49 Recompensa -1 Total rec -264\n",
            "Paso: 76/100\n",
            "Accion 0 Estado 149 Recompensa -1 Total rec -265\n",
            "Paso: 77/100\n",
            "Accion 0 Estado 249 Recompensa -1 Total rec -266\n",
            "Paso: 78/100\n",
            "Accion 3 Estado 229 Recompensa -1 Total rec -267\n",
            "Paso: 79/100\n",
            "Accion 5 Estado 229 Recompensa -10 Total rec -277\n",
            "Paso: 80/100\n",
            "Accion 0 Estado 329 Recompensa -1 Total rec -278\n",
            "Paso: 81/100\n",
            "Accion 1 Estado 229 Recompensa -1 Total rec -279\n",
            "Paso: 82/100\n",
            "Accion 3 Estado 209 Recompensa -1 Total rec -280\n",
            "Paso: 83/100\n",
            "Accion 0 Estado 309 Recompensa -1 Total rec -281\n",
            "Paso: 84/100\n",
            "Accion 2 Estado 309 Recompensa -1 Total rec -282\n",
            "Paso: 85/100\n",
            "Accion 3 Estado 309 Recompensa -1 Total rec -283\n",
            "Paso: 86/100\n",
            "Accion 2 Estado 309 Recompensa -1 Total rec -284\n",
            "Paso: 87/100\n",
            "Accion 0 Estado 409 Recompensa -1 Total rec -285\n",
            "Paso: 88/100\n",
            "Accion 5 Estado 409 Recompensa -10 Total rec -295\n",
            "Paso: 89/100\n",
            "Accion 5 Estado 409 Recompensa -10 Total rec -305\n",
            "Paso: 90/100\n",
            "Accion 0 Estado 409 Recompensa -1 Total rec -306\n",
            "Paso: 91/100\n",
            "Accion 5 Estado 409 Recompensa -10 Total rec -316\n",
            "Paso: 92/100\n",
            "Accion 1 Estado 309 Recompensa -1 Total rec -317\n",
            "Paso: 93/100\n",
            "Accion 3 Estado 309 Recompensa -1 Total rec -318\n",
            "Paso: 94/100\n",
            "Accion 3 Estado 309 Recompensa -1 Total rec -319\n",
            "Paso: 95/100\n",
            "Accion 4 Estado 309 Recompensa -10 Total rec -329\n",
            "Paso: 96/100\n",
            "Accion 5 Estado 309 Recompensa -10 Total rec -339\n",
            "Paso: 97/100\n",
            "Accion 5 Estado 309 Recompensa -10 Total rec -349\n",
            "Paso: 98/100\n",
            "Accion 1 Estado 209 Recompensa -1 Total rec -350\n",
            "Paso: 99/100\n",
            "Accion 2 Estado 229 Recompensa -1 Total rec -351\n",
            "Paso: 100/100\n",
            "Accion 5 Estado 229 Recompensa -10 Total rec -361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-361"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "randomtaxi()"
      ],
      "id": "9PDkmwqj8xZW"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TE0Y9iGi8xZW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "def rltaxi():\n",
        "\n",
        "    # create Taxi environment\n",
        "    env = gym.make('Taxi-v3')\n",
        "\n",
        "    # initialize q-table\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "    qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "    # hyperparameters\n",
        "    learning_rate = 0.9\n",
        "    discount_rate = 0.8\n",
        "    epsilon = 1.0\n",
        "    decay_rate= 0.005\n",
        "\n",
        "    # training variables\n",
        "    num_episodes = 1000\n",
        "    max_steps = 99 # per episode\n",
        "\n",
        "    # training\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # reset the environment\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        for s in range(max_steps):\n",
        "\n",
        "            # exploration-exploitation tradeoff\n",
        "            if random.uniform(0,1) < epsilon:\n",
        "                # explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # exploit\n",
        "                action = np.argmax(qtable[state,:])\n",
        "\n",
        "            # take action and observe reward\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Q-learning algorithm\n",
        "            qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "            # Update to our new state\n",
        "            state = new_state\n",
        "\n",
        "            # if done, finish episode\n",
        "            if done == True:\n",
        "                break\n",
        "\n",
        "        # Decrease epsilon\n",
        "        epsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "    print(f\"Training completed over {num_episodes} episodes\")\n",
        "    input(\"Press Enter to watch trained agent...\")\n",
        "\n",
        "    # watch trained agent\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    for s in range(max_steps+1):\n",
        "\n",
        "        print(f\"TRAINED AGENT\")\n",
        "        print(\"Paso {}\".format(s+1))\n",
        "\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        env.render()\n",
        "        total_reward += reward\n",
        "        print(\"Accion {} Estado {} Recompensa {} Total rec {}\".format(action, state, reward, total_reward))\n",
        "        state = new_state\n",
        "#comentar esto y volver a correr\n",
        "        if done == True:\n",
        "          break\n",
        "\n",
        "    env.close()\n"
      ],
      "id": "TE0Y9iGi8xZW"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kfuys0qO8xZX",
        "outputId": "1f2d29f9-356b-4663-bb16-f128838c6520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed over 1000 episodes\n",
            "Press Enter to watch trained agent...\n",
            "TRAINED AGENT\n",
            "Paso 1\n",
            "Accion 1 Estado 304 Recompensa -1 Total rec -2\n",
            "TRAINED AGENT\n",
            "Paso 2\n",
            "Accion 2 Estado 204 Recompensa -1 Total rec -4\n",
            "TRAINED AGENT\n",
            "Paso 3\n",
            "Accion 2 Estado 224 Recompensa -1 Total rec -6\n",
            "TRAINED AGENT\n",
            "Paso 4\n",
            "Accion 1 Estado 244 Recompensa -1 Total rec -8\n",
            "TRAINED AGENT\n",
            "Paso 5\n",
            "Accion 2 Estado 144 Recompensa -1 Total rec -10\n",
            "TRAINED AGENT\n",
            "Paso 6\n",
            "Accion 2 Estado 164 Recompensa -1 Total rec -12\n",
            "TRAINED AGENT\n",
            "Paso 7\n",
            "Accion 1 Estado 184 Recompensa -1 Total rec -14\n",
            "TRAINED AGENT\n",
            "Paso 8\n",
            "Accion 4 Estado 84 Recompensa -1 Total rec -16\n",
            "TRAINED AGENT\n",
            "Paso 9\n",
            "Accion 0 Estado 96 Recompensa -1 Total rec -18\n",
            "TRAINED AGENT\n",
            "Paso 10\n",
            "Accion 3 Estado 196 Recompensa -1 Total rec -20\n",
            "TRAINED AGENT\n",
            "Paso 11\n",
            "Accion 0 Estado 176 Recompensa -1 Total rec -22\n",
            "TRAINED AGENT\n",
            "Paso 12\n",
            "Accion 3 Estado 276 Recompensa -1 Total rec -24\n",
            "TRAINED AGENT\n",
            "Paso 13\n",
            "Accion 3 Estado 256 Recompensa -1 Total rec -26\n",
            "TRAINED AGENT\n",
            "Paso 14\n",
            "Accion 3 Estado 236 Recompensa -1 Total rec -28\n",
            "TRAINED AGENT\n",
            "Paso 15\n",
            "Accion 1 Estado 216 Recompensa -1 Total rec -30\n",
            "TRAINED AGENT\n",
            "Paso 16\n",
            "Accion 1 Estado 116 Recompensa -1 Total rec -32\n",
            "TRAINED AGENT\n",
            "Paso 17\n",
            "Accion 5 Estado 16 Recompensa 20 Total rec 8\n"
          ]
        }
      ],
      "source": [
        "rltaxi()"
      ],
      "id": "Kfuys0qO8xZX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7858psy8xZY"
      },
      "source": [
        "###### Ejercicio comparar recompensas de ambos metodos con 3 diferentes numeros de iteraciones."
      ],
      "id": "_7858psy8xZY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0fKMtzD8xZZ"
      },
      "outputs": [],
      "source": [],
      "id": "P0fKMtzD8xZZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}